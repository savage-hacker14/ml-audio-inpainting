# blstm_large.yaml

## Dataset config
data:
  dataset: "LibriSpeech"
  root_path: "C:\\Users\\Jacob\\Documents\\2024\\Northeastern\\CS_6140\\Project\\LibriSpeech"
  sample_rate: 16000            # Hz
  train_path: "train-clean-100" # Training subset name of LibriSpeech dataset
  test_path: "test-clean"       # Training subset name of LibriSpeech dataset
  max_len_s: 5.0                # Max audio length in seconds
  gap_len_s: 0.2                # Gap length in seconds
  train_limit: 100              # Limit for training set (for testing purposes)
  gaps_per_audio: 25            # Will make n copies of the audio, with each copy having a gap at a different location 
  spectrogram:
    n_fft: 512                  # FFT window size
    hop_length: 192             # Hop length (Consistent with n_fft/4 common practice)
    win_length: 384             # Window length (same as n_fft)
    window: "hann"              # Window type
    normalize: true             # Normalize spectrograms (log magnitude + simple scaling)
    power: 1.0                  # Use magnitude spectrogram (power=1)

## Model config
model:
  batch_size: 8
  net_dim: [417, 417, 417]    # They used 250 since audio_len was 3s, 48000 samples / 192 (ours is 5s, 80000)
  dropout_rate: 0.0
  max_n_epochs: 50

#### Optimizer config
training:
  optimizer_type: adam
  starter_learning_rate: 0.0001
  lr_decay: 1.0

### Logging and Checkpointing
paths:
  tensorboard_dir: "./tensorboard"      # Directory for Tensorboard logs
  checkpoint_dir: "./checkpoints_NEW"   # Directory for saving checkpoints
  log_dir: "./logs"                     # Directory for saving logs
  sample_dir: "./samples"               # Directory for saving generated samples

logging:
  log_interval: 100         # Log losses every N batches
  checkpoint_interval: 5    # Save checkpoint every N epochs
  run_name: "GAN-board"     # Name for Tensorboard run