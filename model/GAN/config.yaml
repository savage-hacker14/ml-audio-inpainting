# config.yaml
data:
  dataset: "LibriSpeech"
  root_path: "C:\\Users\\Jacob\\Documents\\2024\\Northeastern\\CS_6140\\Project\\LibriSpeech_360" # Added root path
  sample_rate: 16000
  train_path: "train-clean-360"
  valid_path: "dev-clean"
  test_path: "test-clean"
  max_len_s: 5.0           # Max audio length in seconds
  gap_len_s: 0.2           # Gap length in seconds
  train_limit: 3000        # Limit for training set (for testing purposes)
  spectrogram:
    n_fft: 512             # FFT window size
    hop_length: 128        # Hop length (Consistent with n_fft/4 common practice)
    win_length: 512        # Window length (same as n_fft)
    window: "hann"         # Window type
    normalize: true        # Normalize spectrograms (log magnitude + simple scaling)
    power: 1.0             # Use magnitude spectrogram (power=1)

model:
  generator:
    input_channels: 1    # Magnitude
    mask_channels: 1     # Mask (Binary mask for GAN)
    output_channels: 1   # Magnitude
    channels: [64, 128, 256, 512, 512, 512, 512] # Encoder channels from paper
  discriminator:
    input_channels: 1    # Magnitude only
    channels: [64, 128, 256, 512, 1]           # Discriminator channels from paper
    use_spectral_norm: True

# Training Configuration
training:
  batch_size: 8          # Adjust based on GPU memory
  epochs: 100            # Number of training epochs
  g_lr: 0.0002           # Generator learning rate
  d_lr: 0.0002           # Discriminator learning rate
  b1: 0.5                # Adam beta1 (Common for GANs)
  b2: 0.999              # Adam beta2
  # Loss weights based on Eq. 6 (Lp and Ls omitted -> set to 0)
  lambda_adv: 0.01            # Weight for LB (Adversarial loss for Generator)
  lambda_l1_valid: 1.0        # Weight for Lv (L1 loss on valid parts)
  lambda_l1_hole: 2.0         # Weight for Lh (L1 loss on hole parts)
  lambda_vgg_perceptual: 4.0  # Weight for Lp 
  lambda_vgg_style: 500.0     # Weight for Ls
  lambda_mag_weighted: 0.2    # Weight for Lw (Magnitude-weighted L1)
  resume_from_chkpt: true    # Resume from checkpoint 
  resume_run_name: GAN-board_vgg_20250415_232312       # Name of the run to resume from 
  resume_epoch: 45          # Resume from a specific epoch 

# Logging and Checkpointing
paths:
  tensorboard_dir: "./tensorboard"  # Directory for Tensorboard logs
  checkpoint_dir: "./checkpoints"   # Directory for saving checkpoints
  log_dir: "./logs"                 # Directory for saving logs
  sample_dir: "./samples"           # Directory for saving generated samples

logging:
  log_interval: 100      # Log losses every N batches
  checkpoint_interval: 5 # Save checkpoint every N epochs
  sample_interval: 500   # Save audio/spectrogram samples every N batches
  num_workers: 4         # Dataloader workers
  run_name: "GAN-board"  # Name for Tensorboard run